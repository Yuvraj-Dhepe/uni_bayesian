{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dd1a9f-17a2-4ff8-ad20-461d27fbacd4",
   "metadata": {},
   "source": [
    "## Chapter Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96e55c-c619-4681-a27b-4ff3b4b53ba3",
   "metadata": {},
   "source": [
    "### Chapter 1: Prolog n Forecast Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6f7d6-71bb-44ef-81bc-7808f1207414",
   "metadata": {},
   "source": [
    "- 1.1: Physical Processes and observations\n",
    "    - Laplaces' Demon: Zref and it's evolution in time with a map sigh, initial state z^{0}\n",
    "    - Observations: Making Zref noisy and only considering it's partial values, constitutes our observations\n",
    "    - At what time interval ($\\Delta t_{out}$) we have to get obs, given we have number of observations (N_obs) and dt at which we are simulating the Z_ref is given by $\\Delta t_{out} = N_{obs}*\\delta t$. Once we have $\\Delta t_{out}$ we get $K^{th}$ observation by $K * \\Delta t_{out}$ \n",
    "\n",
    "- 1.2: Data Driven Forecasting (Fitting a Polynomial To Data)\n",
    "  - Producing estimate for the future observations, given the present and the past observations\n",
    "  - We do it by fitting a polynomial of order p by making use of p+1 points\n",
    "  - The cf's of the polynomial can be found in different ways, one being the lagrange method\n",
    "  - The cf's can be optimized by splitting the obs in train and test set, where we use residuals to optimize the parameters and settle on one while training\n",
    "\n",
    "- 1.3: Model driven Forecasting and Data Assimilation\n",
    "    - In Model driven forecasting, we use top-down or mechanistic models of the physical process\n",
    "    - Models allows us to obtain estimates of the system state $z(t)$ for $t > t_0 $ given we have the initial state $z(t_0)$\n",
    "    - A model is basically made of parameters, and we have to correct those params over time, by introducing more data at later times\n",
    "    - Data assimilation, which, \n",
    "broadly speaking, covers the task of combining mechanistic models with partial\r\n",
    "and noisy observations in order to produce more skillful forecatss.\n",
    "    - For a model we have to minimize the model error over time, by using the observations that's where data assimilation comes in\n",
    "    - We need methods for estimating appropriate initial condistions for mechanistic model from the available observations (Use of gradient of nonlinear method of least squares)\n",
    "    - Strive to improve the mechanistic models by making the unaccounted contributions from g(t) or model errors as small as possible, so basically improving the models over times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda6978-b8aa-448c-b8f7-a32647d91d78",
   "metadata": {},
   "source": [
    "- For making predictions we simply use the previous predicted points or we use the previous observation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e9ee4-65bc-4a31-838a-e0b99b37ad40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
